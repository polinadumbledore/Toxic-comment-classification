{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открываю таблички с тест и трейн\n",
    "import pandas as pd\n",
    "train_lemm = pd.read_csv('lemm_train_final.csv')\n",
    "test_lemm = pd.read_csv('lemm_test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открываю табличку с y_test\n",
    "test_score = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# табличка с теми тестовыми комментами, которые не учитываются \n",
    "testnotscore = test_score.loc[test_score['toxic'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#дропаю из y_test все то, что не учитывается\n",
    "test_score.drop(test_score[test_score['toxic'] == -1].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#список id, которые не учитываются\n",
    "notscoreid = testnotscore['id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#дропаю все, что не учитывается, из тестового датасета\n",
    "test_lemm = test_lemm[~test_lemm['id'].isin(notscoreid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "5   0001ea8717f6de06      0             0        0       0       0   \n",
       "7   000247e83dcc1211      0             0        0       0       0   \n",
       "11  0002f87b16116a7f      0             0        0       0       0   \n",
       "13  0003e1cccfd5a40a      0             0        0       0       0   \n",
       "14  00059ace3e3e9a53      0             0        0       0       0   \n",
       "\n",
       "    identity_hate  \n",
       "5               0  \n",
       "7               0  \n",
       "11              0  \n",
       "13              0  \n",
       "14              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#целевые переменные в тесте\n",
    "toxic_y_test = test_score['toxic'].to_list()\n",
    "severe_toxic_y_test = test_score['severe_toxic'].to_list()\n",
    "obscene_y_test = test_score['obscene'].to_list()\n",
    "threat_y_test = test_score['threat'].to_list()\n",
    "incult_y_test = test_score['insult'].to_list()\n",
    "identity_hate_y_test = test_score['identity_hate'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевые переменные в трейне\n",
    "toxic_y_train = train_lemm['toxic'].to_list()\n",
    "severe_toxic_y_train = train_lemm['severe_toxic'].to_list()\n",
    "obscene_y_train = test_score['obscene'].to_list()\n",
    "threat_y_train = test_score['threat'].to_list()\n",
    "incult_y_train = test_score['insult'].to_list()\n",
    "identity_hate_y_train = test_score['identity_hate'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#каунт векторайзер\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "cvec = CountVectorizer(max_features=10000) \n",
    "cvec.fit(train_lemm['lemmatized'].values.astype('U'))\n",
    "\n",
    "X_train_1 = cvec.fit_transform(train_lemm['lemmatized'].values.astype('U'))\n",
    "X_test_1 = cvec.transform(test_lemm['lemmatized'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#небольшой гридсерч для дерева решений\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'min_samples_leaf': [2, 3, 4, 5, 6], 'max_depth': [2, 3, 4, 5, 6], 'criterion': ['gini', 'entropy']}\n",
    "\n",
    "est = DecisionTreeClassifier()\n",
    "\n",
    "gridsearch = GridSearchCV(est, params).fit(X_train_1, toxic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшая модель:  {'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 3}\n",
      "точность лучшей модели:  0.9370562330332955\n"
     ]
    }
   ],
   "source": [
    "print('лучшая модель: ', str(gridsearch.best_params_))\n",
    "print('точность лучшей модели: ', gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=3).fit(X_train_1, toxic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194254274906999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test_1, toxic_y_test) #точность на лейбле toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тфидф\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"english\")\n",
    "import numpy as np\n",
    "\n",
    "text_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=200000)\n",
    "X_train_text = text_transformer.fit_transform(train_lemm['lemmatized'].values.astype('U'))\n",
    "X_test_text = text_transformer.transform(test_lemm['lemmatized'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ворд2век\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in train_lemm['lemmatized'].values.astype('U')]\n",
    "w2v_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ворд2век\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train_wv = np.array([vectorize(sentence) for sentence in train_lemm['lemmatized'].values.astype('U')])\n",
    "X_test_wv = np.array([vectorize(sentence) for sentence in test_lemm['lemmatized'].values.astype('U')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#логрег\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=16, max_iter = 1000)\n",
    "x_logreg = logreg.fit(X_train_wv, toxic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предикт\n",
    "y_pred_toxic = logreg.predict(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9254587514458095\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "from sklearn import metrics\n",
    "accuracy_toxic = metrics.accuracy_score( y_pred_toxic, toxic_y_test)\n",
    "print(accuracy_toxic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
